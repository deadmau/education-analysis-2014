{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Team members responsible for this notebook:\n",
      "\n",
      "List the team members contributing to this notebook, along with their responsabilities:\n",
      "\n",
      "* team member 1 **Armando Mota**: team member 1 **Data Gathering & Writing Python Script for gathering tweets and preprocessing**\n",
      "* team member 2 **Elizabeth Sabiniano**: team member 2 **Data Gathering**\n",
      "* team member 3 **Shadman Sadek**: team member 3 **Data Gathering**\n",
      "* team member 4 **Ki Hyun Won**: team member 4 **Data Gathering & Organizing data_gathering.ipynb**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Description of Data Sources:\n",
      "    *Provide the data urls:\n",
      "    \n",
      "        most of the data are collected from https://twitter.com/Twittersearch\n",
      "        \n",
      "    *Describe the file formats: \n",
      "    \n",
      "        We used .pkl, .csv, and .txt formats to store data. \n",
      "        pkl: pickle allowed us to store data without reformatting them to other structures(i.e. DataFrame); provide methods for fast \n",
      "             load and unload of the data\n",
      "        csv: we used comma separated values to classify tweets according to its sentiment: \"positive\", \"neutral\", or \"negative\"\n",
      "        \n",
      "    *Explain the data content of each file: \n",
      "    \n",
      "        **raw data**\n",
      "        withlocation.pkl: tweets that has \"college\" keywords and location field \n",
      "        withoutlocation.pkl tweets that has \"college\" keywords and no location field\n",
      "        \n",
      "        **cleaned data**\n",
      "        allwords.pkl: {'adjectives': frequency} pair for each adjective in withlocation.pkl and withoutlocation.pkl\n",
      "        uswords.pkl: {'adjectives': frequency} pair for each adjective in ustweets.pkl\n",
      "        worldwords.pkl: {'adjectives': frequency} pair for each adjective in worldtweets.pkl\n",
      "        ustweets.pkl: tweets that has \"college\" keywords and its location in U.S.\n",
      "        worldtweets.pkl: tweets that has \"college\" keywords and its location non-U.S.\n",
      "        \n",
      "        **cleaned data only for tweet sentiment analysis**\n",
      "        *sentiment_type: \"positive\", \"negative\", or \"neutral\"\n",
      "        training_dataset.csv: A list of 490 manually classified tweets according to their sentiment_type\n",
      "        featureList.pkl: A list of {'words': sentiment_type} pair that initially classifies each word in training_data.csv \n",
      "        stopwords.txt: A list of words that are meaningless in sentiment analysis (i.e. about, the, a, etc...)\n",
      "        classifier.pkl: A Bayes Net classifier that determines the sentiment of a new tweet "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Creating the directory structure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "mkdir ../script ../data ../data/raw ../data/cleaned ../data/simulated ../visualizations\n",
      "ls -r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "README.md\n",
        "NB4_project_report.ipynb\n",
        "NB3_data_analysis.ipynb\n",
        "NB2_data_cleaning.ipynb\n",
        "NB1_data_gathering.ipynb\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Downloading Raw Data\n",
      "\n",
      "**Since gathering thousands of tweets takes long time, we are only going to show how we gathered data in python script**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file ../script/StreamAnalysis/stream.py\n",
      "\n",
      "from twython import TwythonStreamer\n",
      "import pickle\n",
      "\n",
      "\n",
      "\n",
      "# set up keys\n",
      "consumer_key = 'uVIphnOOjUHubKdicEkA'\n",
      "consumer_secret = '31VcB8ZIunTkSDWnHyJosEMrkNpNy9ecQigxOMruSk'\n",
      "access_token = '2419638589-5THAmW79kgGzcF3USxmi4kK0KA4UiUBzCMFxI6x'\n",
      "access_token_secret = 'Pkb4ZPHODZ4ucDoKMPtzjbehlffIX6dUj84Yt5UmJbOGU'\n",
      "\n",
      "# set up set and dictionary to locally store tweets and words until external file save\n",
      "locText = {}\n",
      "noLocText = set()\n",
      "oglocsize = 0\n",
      "ognolocsize = 0\n",
      "filename1 = '../../data/raw/withlocation.pkl'\n",
      "filename2 = '../../data/raw/withoutlocation.pkl'\n",
      "\n",
      "\n",
      "# load previous set/dictionary data into textSet and wordDict if they exist\n",
      "def loadFiles(file1, file2):\n",
      "    global locText\n",
      "    global noLocText\n",
      "    global oglocsize\n",
      "    global ognolocsize\n",
      "    try:\n",
      "        locFile = open(file1, 'rb')\n",
      "        noLocFile = open(file2, 'rb')\n",
      "        locText = pickle.load(locFile)\n",
      "        noLocText = pickle.load(noLocFile)\n",
      "        oglocsize = len(locText)\n",
      "        ognolocsize = len(noLocText)\n",
      "        locFile.close()\n",
      "        noLocFile.close()\n",
      "    except BaseException:\n",
      "        print \"One or both files were empty\"\n",
      "        pass\n",
      "\n",
      "# makes the magic happen\n",
      "class MyStreamer(TwythonStreamer):\n",
      "    def on_success(self, data):\n",
      "        if 'text' in data:\n",
      "            text = data['text'].encode('utf-8')\n",
      "            if text.find('http') != -1:\n",
      "                    text = text[:text.find('http')-1]            \n",
      "            if text not in locText and text not in noLocText and text.find('college') != -1:\n",
      "                # print text\n",
      "                if data['place'] != None:\n",
      "                    locText[text] = (data['place']['country_code']).encode('ascii','ignore')\n",
      "                else:\n",
      "                    if data['coordinates'] != None:\n",
      "                        locText[text] = data['coordinates']['coordinates']\n",
      "                    else:\n",
      "                        noLocText.add(text)\n",
      "                \n",
      "            \n",
      "    def on_error(self, status_code, data):\n",
      "        print status_code\n",
      "        self.disconnect()\n",
      "\n",
      "\n",
      "# run it\n",
      "try :\n",
      "    loadFiles(filename1, filename2)\n",
      "    locWrite = open(filename1, 'wb')\n",
      "    noLocWrite = open(filename2, 'wb')      \n",
      "    stream = MyStreamer(consumer_key, consumer_secret, access_token, access_token_secret)\n",
      "    # this line actually starts the streaming, with college as the keyword\n",
      "    stream.statuses.filter(track='college')\n",
      "except (KeyboardInterrupt, SystemExit):\n",
      "    # when you hit Ctrl^C, this catches the exception and writes everything that has been\n",
      "    # added to the set and dictionary to the two files specified above.  YOU MUST ONLY STOP\n",
      "    #THIS SCRIPT WITH CTRL^C OR ELSE YOU WON'T SAVE ANY OF THE DATA YOU'VE COLLECTED IN THE\n",
      "    #CURRENT SESSION\n",
      "    pickle.dump(locText, locWrite)\n",
      "    pickle.dump(noLocText, noLocWrite)\n",
      "    locWrite.close()\n",
      "    noLocWrite.close()\n",
      "    print ''\n",
      "    print 'Gathered ' + str(len(locText) - oglocsize) + ' tweets with locations.'\n",
      "    print 'Gathered ' + str(len(noLocText) - ognolocsize) + ' tweets with no locations.'\n",
      "    print ''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting ../script/StreamAnalysis/stream.py\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gathered Data are stored in two files: withlocation.pkl and withoutlocation.pkl**\n",
      "\n",
      "**Here, we preprocessed raw data as we are gathering them: tweet with location and tweet without location**\n",
      "\n",
      "###Displaying raw data\n",
      "    *Only shown few sample of the data*\n",
      "    *Here the raw data is stored are (key, value) pair.\n",
      "     The key is tweet and the value is location.\n",
      "     Since withoutlocation.pkl does not have location field, it only stores tweets as a list."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "#load collected data\n",
      "locFile = pd.read_pickle('../data/raw/withlocation.pkl')\n",
      "noLocFile = pd.read_pickle('../data/raw/withoutlocation.pkl')\n",
      "counter = 0\n",
      "print \"Printing 5 raw tweets with location out of \"+ str(len(locFile))+\" tweets \\n\"\n",
      "for key in locFile.keys():\n",
      "    print \"Text: \" + key + \", Location: \" + locFile[key] + '\\n'\n",
      "    counter += 1\n",
      "    if counter == 5: break\n",
      "\n",
      "print \"\\nPrinting 5 raw tweets without location out of \" + str(len(noLocFile))+\" tweets \\n\"\n",
      "for text in list(noLocFile):\n",
      "    print \"Text: \" + text + \"\\n\"\n",
      "    counter += 1\n",
      "    if counter == 10: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Printing 5 raw tweets with location out of 18840 tweets \n",
        "\n",
        "Text: waking up at 8 pm on a sunday. am i doing college right or wrong?, Location: US\n",
        "\n",
        "Text: sometimes I feel like my mom is more excited about college then me, Location: US\n",
        "\n",
        "Text: @theonlytyjones I humbly regret to inform you that graduating college was my biggest mistake ever.... #Yolo #School4Life, Location: US\n",
        "\n",
        "Text: My brother comes home today from college, pumped to see him!, Location: US\n",
        "\n",
        "Text: Just finished first semester of college!! #amazing #uf, Location: US\n",
        "\n",
        "\n",
        "Printing 5 raw tweets without location out of 350309 tweets \n",
        "\n",
        "Text: Severely missing two of my best buds in the world come back from college already!!!!! @BP_Parker25 and @IAmAGing\n",
        "\n",
        "Text: RT @ChrisUNODenton: 12 days till I walk across the stage &amp; be officially done with college!!!!!!!!\n",
        "\n",
        "Text: \ud83d\ude10 \u201c@CountOn24: better college player: kobe or lebron?\u201d\n",
        "\n",
        "Text: all im saying is the same people ive been seeing since i been in college better be the same people i see this summer \ud83d\ude01\ud83d\ude18\n",
        "\n",
        "Text: You just have ruined our next three years of college\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Combining tweets from https://github.com/ravikiranj/twitter-sentiment-analyzer/tree/master/data and the gathered tweets above, we manually classified 490 tweets as a sample for sentiment analysis later**\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load collected data\n",
      "f = pd.read_csv('../data/raw/training_dataset.csv')\n",
      "f.columns = ['sentiment', 'text']\n",
      "print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "   sentiment                                               text\n",
        "0   positive                            tailored to entertain !\n",
        "1   positive                       a vivid cinematic portrait .\n",
        "2   positive   hilarious , touching and wonderfully dyspeptic .\n",
        "3   positive                        a smart , witty follow-up .\n",
        "4   positive  while the ideas about techno-saturation are fa...\n",
        "5   positive  an infectious cultural fable with a tasty bala...\n",
        "6   positive  although occasionally static to the point of r...\n",
        "7   positive  it provides an honest look at a community stri...\n",
        "8   positive  add yet another hat to a talented head , cloon...\n",
        "9   positive  building slowly and subtly , the film , sporti...\n",
        "10  positive  ultimately feels empty and unsatisfying , like...\n",
        "11  positive  chilling , well-acted , and finely directed : ...\n",
        "12  positive  a swashbuckling tale of love , betrayal , reve...\n",
        "13  positive                             . . . a true delight .\n",
        "14  positive  for the most part stevens glides through on so...\n",
        "15  positive  broomfield turns his distinctive 'blundering' ...\n",
        "16  positive  against all odds in heaven and hell , it creep...\n",
        "17  positive  it's refreshing to see a girl-power movie that...\n",
        "18  positive  it's worth seeing just on the basis of the wis...\n",
        "19  positive  a rigorously structured and exquisitely filmed...\n",
        "20  positive  this surreal gilliam-esque film is also a trou...\n",
        "21  positive         a quiet treasure -- a film to be savored .\n",
        "22  positive  may be far from the best of the series , but i...\n",
        "23  positive  a compelling spanish film about the withering ...\n",
        "24  positive  huston nails both the glad-handing and the cho...\n",
        "25  positive  may not have generated many sparks , but with ...\n",
        "26  positive     a delirious celebration of the female orgasm .\n",
        "27  positive  exquisitely nuanced in mood tics and dialogue ...\n",
        "28  positive  it's fascinating to see how bettany and mcdowe...\n",
        "29  positive  the film is beautifully mounted , but , more t...\n",
        "30  positive  leigh's film is full of memorable performances...\n",
        "31  positive  one of the most significant moviegoing pleasur...\n",
        "32  positive  jose campanella delivers a loosely autobiograp...\n",
        "33  positive  generally , clockstoppers will fulfill your wi...\n",
        "34  positive  the movie is beautiful to behold and engages o...\n",
        "35  positive  ao sair do cinema , eu estava feliz e com saud...\n",
        "36  positive  neither parker nor donovan is a typical romant...\n",
        "37  positive  it's a much more emotional journey than what s...\n",
        "38  positive  not only are the special effects and narrative...\n",
        "39  positive  jaglom . . . put[s] the audience in the privil...\n",
        "40  positive  beautifully observed , miraculously unsentimen...\n",
        "41  positive  a must-see for the david mamet enthusiast and ...\n",
        "42  positive  crackerjack entertainment -- nonstop romance ,...\n",
        "43  positive  the acting , costumes , music , cinematography...\n",
        "44  positive  garc\ufffda bernal and talanc_n are an immensely ap...\n",
        "45  positive  far more imaginative and ambitious than the tr...\n",
        "46  positive  the very definition of the 'small' movie , but...\n",
        "47  positive  a gripping , searing portrait of a lost soul t...\n",
        "48  positive  suffers from the lack of a compelling or compr...\n",
        "49  positive  so unassuming and pure of heart , you can't he...\n",
        "50  positive  an intriguing cinematic omnibus and round-robi...\n",
        "51  positive  so refreshingly incisive is grant that for the...\n",
        "52  positive  at a time when half the so-called real movies ...\n",
        "53  positive  the magic of the film lies not in the mysterio...\n",
        "54  positive  hoffman notches in the nuances of pain , but h...\n",
        "55  positive  what better message than 'love thyself' could ...\n",
        "56  positive  the second coming of harry potter is a film fa...\n",
        "57  positive  84 minutes of rolling musical back beat and su...\n",
        "58  positive  . . . takes the beauty of baseball and melds i...\n",
        "59  positive  seldahl's barbara is a precise and moving port...\n",
        "         ...                                                ...\n",
        "\n",
        "[489 rows x 2 columns]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**From https://github.com/ravikiranj/twitter-sentiment-analyzer/tree/master/data, we obtained \"stopwords\" (i.e. words that has no sentiment value) to later parse them out for sentiment analysis**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#load collected data\n",
      "f = pd.read_csv('../data/raw/stopwords.txt')\n",
      "print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "            \n",
        "0          a\n",
        "1      about\n",
        "2      above\n",
        "3     across\n",
        "4      after\n",
        "5      again\n",
        "6    against\n",
        "7        all\n",
        "8     almost\n",
        "9      alone\n",
        "10     along\n",
        "11   already\n",
        "12      also\n",
        "13  although\n",
        "14    always\n",
        "15     among\n",
        "16        an\n",
        "17       and\n",
        "18   another\n",
        "19       any\n",
        "20   anybody\n",
        "21    anyone\n",
        "22  anything\n",
        "23  anywhere\n",
        "24       are\n",
        "25      area\n",
        "26     areas\n",
        "27    around\n",
        "28        as\n",
        "29       ask\n",
        "30     asked\n",
        "31    asking\n",
        "32      asks\n",
        "33        at\n",
        "34      away\n",
        "35         b\n",
        "36      back\n",
        "37    backed\n",
        "38   backing\n",
        "39     backs\n",
        "40        be\n",
        "41    became\n",
        "42   because\n",
        "43    become\n",
        "44   becomes\n",
        "45      been\n",
        "46    before\n",
        "47     began\n",
        "48    behind\n",
        "49     being\n",
        "50    beings\n",
        "51      best\n",
        "52    better\n",
        "53   between\n",
        "54       big\n",
        "55      both\n",
        "56       but\n",
        "57        by\n",
        "58         c\n",
        "59      came\n",
        "         ...\n",
        "\n",
        "[423 rows x 1 columns]\n"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}